name: Create Global Infrastructure (Manual Workflow)

on:
  workflow_dispatch:
    branches:
      - main
    inputs:
      shouldCreateEKS:
        description: "Do you want to create EKS cluster?"
        type: boolean
      shouldCreateEFS:
        description: "Do you want to create EFS?"
        type: boolean
      shouldCreateALB:
        description: "Do you want to create ALB?"
        type: boolean
      shouldCreateRDS:
        description: "Do you want to create RDS?"
        type: boolean

jobs:
  create-global-infrastructure:
    name: Create Global Infrastructure (Manual Workflow)
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{vars.REGION}}

      - name: Install eksctl
        run: |
          ARCH=amd64
          PLATFORM=$(uname -s)_$ARCH
          curl -sLO "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz"
          # (Optional) Verify checksum
          curl -sL "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_checksums.txt" | grep $PLATFORM | sha256sum --check
          tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp && rm eksctl_$PLATFORM.tar.gz
          sudo mv /tmp/eksctl /usr/local/bin

      - name: Create EKS Cluster
        if: inputs.shouldCreateEKS
        run: |
          eksctl create cluster -f <(echo '
             apiVersion: eksctl.io/v1alpha5
             kind: ClusterConfig

             metadata:
               name: ${{vars.CLUSTER}}
               region: ${{vars.REGION}}

             fargateProfiles:
               - name: fp-${{vars.CLUSTER}}-${{vars.NAMESPACE}}
                 selectors:
                   - namespace: ${{vars.NAMESPACE}}
                   - namespace: kube-system
           ')

      - name: Create EFS & Dependencies
        if: inputs.shouldCreateEFS
        run: |
          eksctl utils write-kubeconfig --cluster ${{vars.CLUSTER}} --region ${{vars.REGION}}
          
          VPC_ID=$(aws eks describe-cluster --name ${{vars.CLUSTER}} --query "cluster.resourcesVpcConfig.vpcId" --region ${{vars.REGION}} --output text)
          CIDR_BLOCK=$(aws ec2 describe-vpcs --vpc-ids $VPC_ID --query "Vpcs[].CidrBlock" --region ${{vars.REGION}} --output text)
          
          # EFS File System
          EFS_FS_ID=$(aws efs create-file-system \
            --creation-token efs-${{vars.CLUSTER}} \
            --encrypted \
            --performance-mode generalPurpose \
            --throughput-mode bursting \
            --tags Key=Name,Value=efs-${{vars.CLUSTER}} \
            --region ${{vars.REGION}} \
            --output text \
            --query "FileSystemId")
          
          # Wait until EFS File System is available, with a maximum wait time of 150 seconds
            wait_time=0
            while [ $wait_time -lt 150 ]; do
              status=$(aws efs describe-file-systems \
              --file-system-id $EFS_FS_ID \
              --region ${{vars.REGION}} \
              --output text \
              --query "FileSystems[0].LifeCycleState")
          
              if [ "$status" = "available" ]; then
                break
              else
                echo "Waiting for EFS File System to become available..."
                sleep 10  # Wait for 10 seconds before checking again
              wait_time=$((wait_time + 10))
              fi
            done

          echo "EFS File System created"

          # Security Group for File System for Inbound traffic
          EFS_SG_ID=$(aws ec2 create-security-group \
            --description "Security Group for File System for Inbound traffic" \
            --group-name eks-efs-${{vars.CLUSTER}} \
            --vpc-id $VPC_ID \
            --region ${{vars.REGION}} \
            --query 'GroupId' --output text)
          
          aws ec2 authorize-security-group-ingress \
            --group-id $EFS_SG_ID \
            --protocol tcp \
            --port 2049 \
            --cidr $CIDR_BLOCK
          
          echo "Security group created"
          
          # Create EFS mount targets for the volume in all subnets used in the Fargate profile
          for subnet in $(aws eks describe-fargate-profile \
            --output text --cluster-name ${{vars.CLUSTER}} \
            --fargate-profile-name fp-${{vars.CLUSTER}}-${{vars.NAMESPACE}}  \
            --region ${{vars.REGION}}  \
            --query "fargateProfile.subnets"); \
          do (aws efs create-mount-target \
            --file-system-id $EFS_FS_ID \
            --subnet-id $subnet \
            --security-group $EFS_SG_ID \
            --region ${{vars.REGION}}); \
          done
          
          echo "EFS mount targets done"

      - name: Create ALB (External Facing)
        if: inputs.shouldCreateALB
        run: |
          eksctl utils write-kubeconfig --cluster ${{vars.CLUSTER}} --region ${{vars.REGION}}
          
          ACCOUNT_ID=$(aws sts get-caller-identity --query 'Account' --output text)
          VPC_ID=$(aws eks describe-cluster --name ${{vars.CLUSTER}} --query "cluster.resourcesVpcConfig.vpcId" --region ${{vars.REGION}} --output text)
          
          ## Associate OIDC provider
          eksctl utils associate-iam-oidc-provider \
            --region ${{vars.REGION}} \
            --cluster ${{vars.CLUSTER}} \
            --approve
          
          POLICY_NAME="AWSLoadBalancerControllerIAMPolicy"
      
          # Check if the policy already exists
          if ! aws iam get-policy --policy-arn "arn:aws:iam::$ACCOUNT_ID:policy/$POLICY_NAME" >/dev/null 2>&1; then
            ## Download the IAM policy document
            curl -S https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.6.0/docs/install/iam_policy.json -o iam-policy.json
            aws iam create-policy --policy-name $POLICY_NAME --policy-document file://iam-policy.json
            echo "IAM Policy created: $policy_name"
          else
            echo "IAM Policy already exists: $POLICY_NAME"
          fi
          
          ## Create a service account
          eksctl create iamserviceaccount \
            --cluster=${{vars.CLUSTER}} \
            --region ${{vars.REGION}} \
            --namespace=kube-system \
            --name=aws-load-balancer-controller \
            --override-existing-serviceaccounts \
            --attach-policy-arn=arn:aws:iam::$ACCOUNT_ID:policy/AWSLoadBalancerControllerIAMPolicy \
            --approve
          
          ## The AWS Load Balancer Controller uses cert-manager
          eksctl create fargateprofile \
            --cluster ${{vars.CLUSTER}} \
            --name cert-manager \
            --namespace cert-manager \
            --region ${{vars.REGION}}
          
      - name: helm deploy aws-load-balancer-controller
        if: inputs.shouldCreateALB
        uses: bitovi/github-actions-deploy-eks-helm@v1.2.4
        with:
          aws-region: ${{vars.REGION}}
          cluster-name: ${{vars.CLUSTER}}
          chart-repository: https://aws.github.io/eks-charts
          chart-path: stable/aws-load-balancer-controller
          namespace: kube-system
          values: clusterName=${{vars.CLUSTER}},serviceAccount.create=false,serviceAccount.name=aws-load-balancer-controller,vpcId=$VPC_ID,region=${{vars.REGION}}
          name: aws-load-balancer-controller

      - name: Create RDS (MySQL)
        if: inputs.shouldCreateRDS
        run: |
          eksctl utils write-kubeconfig --cluster ${{vars.CLUSTER}} --region ${{vars.REGION}}
          
          ## Get VPC's private subnets
          FARGATE_PRIVATE_SUBNETS=$(aws eks describe-fargate-profile \
            --fargate-profile-name fp-${{vars.CLUSTER}}-${{vars.NAMESPACE}}  \
            --cluster-name ${{vars.CLUSTER}} \
            --region ${{vars.REGION}} \
            --query "fargateProfile.[subnets]" --output text | awk -v OFS="," '{for(i=1;i<=NF;i++)if($i~/subnet/)$i="\"" $i "\"";$1=$1}1')
          
          ## Create a DB subnet group
          aws rds create-db-subnet-group \
            --db-subnet-group-name fp-${{vars.CLUSTER}}-${{vars.NAMESPACE}}-db-subnet \
            --subnet-ids "[$FARGATE_PRIVATE_SUBNETS]" \
            --db-subnet-group-description "Subnet group for MySQL RDS" \
            --region ${{vars.REGION}}
          
          ## Create database instance
          aws rds create-db-instance \
            --db-instance-identifier ${{vars.CLUSTER}}-db \
            --db-instance-class db.t3.micro \
            --db-name ${{secrets.MYSQL_DB_NAME}} \
            --db-subnet-group-name fp-${{vars.CLUSTER}}-${{vars.NAMESPACE}}-db-subnet \
            --engine mysql \
            --master-username ${{secrets.MYSQL_MASTER_USER}}  \
            --master-user-password ${{secrets.MYSQL_MASTER_PASSWORD}} \
            --allocated-storage 20 \
            --no-publicly-accessible \
            --region ${{vars.REGION}}
          
          # Wait for RDS to be available it can take upto 5 mins.
          while true; do
            status=$(aws rds describe-db-instances --db-instance-identifier "${{vars.CLUSTER}}-db" --query "DBInstances[0].DBInstanceStatus" --output text --region "${{vars.REGION}}")
            if [[ "$status" == "available" ]]; then
              echo "RDS instance is now available."
              break
            else
              echo "Waiting for RDS instance to become available (current status: $status)..."
              sleep 30
            fi
          done

          ## Get the security group attached to the RDS instance
          RDS_SG=$(aws rds describe-db-instances \
              --db-instance-identifier ${{vars.CLUSTER}}-db \
              --region ${{vars.REGION}} \
              --query "DBInstances[].VpcSecurityGroups[].VpcSecurityGroupId" \
              --output text)
          
          VPC_ID=$(aws eks describe-cluster --name ${{vars.CLUSTER}} --query "cluster.resourcesVpcConfig.vpcId" --region ${{vars.REGION}} --output text)
          CIDR_BLOCK=$(aws ec2 describe-vpcs --vpc-ids $VPC_ID --query "Vpcs[].CidrBlock" --region ${{vars.REGION}} --output text)
          
          ## Accept MySQL traffic
          aws ec2 authorize-security-group-ingress \
            --group-id $RDS_SG \
            --cidr $CIDR_BLOCK \
            --port 3306 \
            --protocol tcp \
            --region ${{vars.REGION}}
