name: Create Global Infrastructure (Manual Workflow)

on:
  workflow_dispatch:
    branches:
      - main
    inputs:
      shouldCreateEKS:
        description: "Do you want to create EKS cluster?"
        type: boolean
      shouldCreateEFS:
        description: "Do you want to create EFS?"
        type: boolean
      shouldCreateALB:
        description: "Do you want to create ALB?"
        type: boolean

jobs:
  create-global-infrastructure:
    name: Create Global Infrastructure (Manual Workflow)
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{vars.REGION}}

      - name: Install eksctl
        run: |
          ARCH=amd64
          PLATFORM=$(uname -s)_$ARCH
          curl -sLO "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz"
          # (Optional) Verify checksum
          curl -sL "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_checksums.txt" | grep $PLATFORM | sha256sum --check
          tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp && rm eksctl_$PLATFORM.tar.gz
          sudo mv /tmp/eksctl /usr/local/bin

      - name: Create EKS Cluster
        if: inputs.shouldCreateEKS
        run: |
          eksctl create cluster -f <(echo '
             apiVersion: eksctl.io/v1alpha5
             kind: ClusterConfig

             metadata:
               name: ${{vars.CLUSTER}}
               region: ${{vars.REGION}}

             fargateProfiles:
               - name: fp-${{vars.CLUSTER}}-${{vars.NAMESPACE}}
                 selectors:
                   - namespace: ${{vars.NAMESPACE}}
                   - namespace: kube-system
           ')

      - name: Create EFS & Dependencies
        if: inputs.shouldCreateEFS
        run: |
          eksctl utils write-kubeconfig --cluster ${{vars.CLUSTER}} --region ${{vars.REGION}}
          
          VPC_ID=$(aws eks describe-cluster --name ${{vars.CLUSTER}} --query "cluster.resourcesVpcConfig.vpcId" --region ${{vars.REGION}} --output text)
          CIDR_BLOCK=$(aws ec2 describe-vpcs --vpc-ids $VPC_ID --query "Vpcs[].CidrBlock" --region ${{vars.REGION}} --output text)
          
          # EFS File System
          EFS_FS_ID=$(aws efs create-file-system \
            --creation-token efs-${{vars.CLUSTER}} \
            --encrypted \
            --performance-mode generalPurpose \
            --throughput-mode bursting \
            --tags Key=Name,Value=efs-${{vars.CLUSTER}} \
            --region ${{vars.REGION}} \
            --output text \
            --query "FileSystemId")
          
          # Wait until EFS File System is available, with a maximum wait time of 150 seconds
            wait_time=0
            while [ $wait_time -lt 150 ]; do
              status=$(aws efs describe-file-systems \
              --file-system-id $EFS_FS_ID \
              --region ${{vars.REGION}} \
              --output text \
              --query "FileSystems[0].LifeCycleState")
          
              if [ "$status" = "available" ]; then
                break
              else
                echo "Waiting for EFS File System to become available..."
                sleep 10  # Wait for 10 seconds before checking again
              wait_time=$((wait_time + 10))
              fi
            done

          echo "EFS File System created"

          # Security Group for File System for Inbound traffic
          EFS_SG_ID=$(aws ec2 create-security-group \
            --description "Security Group for File System for Inbound traffic" \
            --group-name eks-efs-${{vars.CLUSTER}} \
            --vpc-id $VPC_ID \
            --region ${{vars.REGION}} \
            --query 'GroupId' --output text)
          
          aws ec2 authorize-security-group-ingress \
            --group-id $EFS_SG_ID \
            --protocol tcp \
            --port 2049 \
            --cidr $CIDR_BLOCK
          
          echo "Security group created"
          
          # Create EFS mount targets for the volume in all subnets used in the Fargate profile
          for subnet in $(aws eks describe-fargate-profile \
            --output text --cluster-name ${{vars.CLUSTER}} \
            --fargate-profile-name fp-${{vars.CLUSTER}}-${{vars.NAMESPACE}}  \
            --region ${{vars.REGION}}  \
            --query "fargateProfile.subnets"); \
          do (aws efs create-mount-target \
            --file-system-id $EFS_FS_ID \
            --subnet-id $subnet \
            --security-group $EFS_SG_ID \
            --region ${{vars.REGION}}); \
          done
          
          echo "EFS mount targets done"

      - name: Create ALB (External Facing)
        if: inputs.shouldCreateALB
        run: |
          eksctl utils write-kubeconfig --cluster ${{vars.CLUSTER}} --region ${{vars.REGION}}
          
          ACCOUNT_ID=$(aws sts get-caller-identity --query 'Account' --output text)
          VPC_ID=$(aws eks describe-cluster --name ${{vars.CLUSTER}} --query "cluster.resourcesVpcConfig.vpcId" --region ${{vars.REGION}} --output text)
          
          ## Associate OIDC provider
          eksctl utils associate-iam-oidc-provider \
            --region ${{vars.REGION}} \
            --cluster ${{vars.CLUSTER}} \
            --approve
          
          ## Download the IAM policy document
          curl -S https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json -o iam-policy.json
          
          ## Create an IAM policy
          aws iam create-policy \
            --policy-name AWSLoadBalancerControllerIAMPolicy \
            --policy-document file://iam-policy.json
          
          ## Create a service account
          eksctl create iamserviceaccount \
            --cluster=${{vars.CLUSTER}} \
            --region ${{vars.REGION}} \
            --namespace=kube-system \
            --name=aws-load-balancer-controller \
            --override-existing-serviceaccounts \
            --attach-policy-arn=arn:aws:iam::$ACCOUNT_ID:policy/AWSLoadBalancerControllerIAMPolicy \
            --approve
          
          ## The AWS Load Balancer Controller uses cert-manager
          eksctl create fargateprofile \
            --cluster ${{vars.CLUSTER}} \
            --name cert-manager \
            --namespace cert-manager \
            --region ${{vars.REGION}}
          
      - name: helm deploy
        if: inputs.shouldCreateALB
        uses: bitovi/github-actions-deploy-eks-helm@v1.2.4
        with:
          aws-region: ${{vars.REGION}}
          cluster-name: ${{vars.CLUSTER}}
          chart-repository: https://aws.github.io/eks-charts
          chart-path: stable/aws-load-balancer-controller
          namespace: kube-system
          values: clusterName=${{vars.CLUSTER}},serviceAccount.create=false,serviceAccount.name=aws-load-balancer-controller,vpcId=$VPC_ID,region=${{vars.REGION}}
          name: aws-load-balancer-controller
